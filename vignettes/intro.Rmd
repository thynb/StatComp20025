---
title: "Homework"
author: "Hanyang Tian"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to Homework}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
# Homework0

## The 1st example

<font size=4>

* Question

<font size=3>

The 1st example comes from the **Example 3.2**, *page 50 of the book "Statistical Computing with R"*.

This example uses the **inverse transform method** to simulate a random sample
from the distribution with density $f_X(x) = 3x^2, 0 < x < 1$.

Here $F_X(x) = x^3$ for $0 < x < 1$, and $F_X^{−1}(u) = u^{1/3}$. Generate all n required random uniform numbers as vector $u$. Then $u^{(1/3)}$ is a vector of length n containing the sample $x_1, . . . , x_n$.

<font size=4>

* Answer

<font size=3>
```{r  }
n <- 10000
u <- runif(n)
x <- u^(1/3)
hist(x, density = 3,breaks = 20,col="red",prob = TRUE) #density histogram of sample
y <- seq(0, 1, .01)
lines(y, 3*y^2,col='black') #density curve f(x)

```

## The 2st example
<font size=4>

* Question

<font size=3>
The 2st example comes from the **Example 1.6**, *page 14 of the book "Statistical Computing with R"*.

This example creates a list to assign row and column names in a matrix. The
first component for row names will be **NULL** in this case because we do not
want to assign row names.

<font size=4>

* Answer

<font size=3>
```{r }
a <- matrix(runif(8), 4, 2) #a 4x2 matrix

dimnames(a) <- list(NULL, c("X", "Y"))
knitr::kable(a,align='c',caption='Matrix1')

#a <- matrix(runif(8), 4, 2) #a 4x2 matrix

dimnames(a) <- list(letters[1:4], c("X", "Y"))
knitr::kable(a,align='c',caption='Matrix2')

row.names(a) <- list("NE", "NW", "SW", "SE")
knitr::kable(a,align='c',caption='Matrix3')
```

## The 3st example
<font size=4>

* Question

<font size=3>
The 3st example comes from the **Example 5.1(Simple Monte Carlo integration)**, *page 120 of the book "Statistical Computing with R"*.

This example considers the problem of estimating $\theta =\int_{0}^{1} g(x) {\rm d}x$. If $X_1, . . . , X_m$ is a random Uniform(0,1) sample then

$$\hat{\theta} = \overline{g_m(X)} =\frac{1}{m} \sum_{k=1}^N g(X_i)$$
converges to $E[g(X)] = θ$ with probability 1, by the Strong Law of Large
Numbers. The simple Monte Carlo estimator of $\int_{0}^{1} g(x){\rm d}x$ is $\overline{g_m(x)}$.


Then we compute a Monte Carlo estimate of
$$\theta =\int_{0}^{1} e^{-x} {\rm d}x$$
and compare the estimate with the exact value.


<font size=5>

* Answer

<font size=3>
```{r }

m <- 10000
x <- runif(m, min=2, max=4)
theta.hat <- mean(exp(-x)) * 2
print(theta.hat)
print(exp(-2) - exp(-4))

```

# Homework1

## Exercises 3.3

<font size=5>

* Question

<font size=4>

The **Pareto(a, b)** distribution has cdf

$$F(x) = 1-\left( \frac{b}{x} \right)^a,\quad x\geq b>0,a>0$$

Derive the probability inverse transformation $F^{-1}(U)$ and use the inverse
transform method to simulate a random sample from the **Pareto(2, 2)** distribution. Graph the density histogram of the sample with the **Pareto(2, 2)**
density superimposed for comparison.
<font size=5>

* Answer

<font size=4>

By doing the math we can figure out the 
$$F^{-1}(U)=\frac{b}{\sqrt[a]{1-U}}$$


Then we compute the density function of **Pareto(a, b)**.

$$f(x)=F'(x)=a\times b\times x^{-(a+1)}$$

```{r}


f_i=function(u,a,b){
  f1=b
  f2=(1-u)^(1/a)
  return(f1/f2)
}

n <- 1000
u <- runif(n)

x <- f_i(u,2,2)
hist(x, density = 3,breaks=50, col="red",prob = TRUE) #density histogram of sample

f=function(x,a,b){
  x1=x^(-a-1)
  return(a*b*x1)
}

y <- seq(2,max(x),0.01)

lines(y, f(y,2,2),col='blue')

```


## Exercises 3.9
<font size=5>

* Question

<font size=4>
The rescaled Epanechnikov kernel is a symmetric density function

$$f_e(x)=\frac{3}{4}(1-x^2),\quad |x|\leq 1$$
Devroye and Gyorfi give the following algorithm for simulation
from this distribution. Generate iid $U1, U2, U3 \sim \mathbf{Uniform}(−1, 1)$. If $|U3| ≥|U2|$ and $|U3| ≥ |U1|$, deliver $U2$; otherwise deliver $U3$. Write a function
to generate random variates from $f_e$, and construct the histogram density
estimate of a large simulated random sample.

<font size=5>

* Answer

<font size=4>
```{r }
x=c()
for (i in 1:10000) {
  U=runif(3,-1,1)
  if( abs(U[3])>=abs(U[2]) && abs(U[3])>=abs(U[1])){
    x=c(x,U[2])
  }
  else{
    x=c(x,U[3])
  }
}

hist(x, density = 3,breaks=30, col="red",prob = TRUE) #density histogram of sample

y <- seq(-1,1,0.01)

f_e=function(x){
  return(3/4*(1-x^2))
}
lines(y, f_e(y),col='blue')
```

## Exercises 3.10

<font size=4>

* Question

<font size=3>
Prove that the algorithm given in Exercise 3.9 generates variates from the
density $f_e (3.10)$


<font size=4>

* Answer

**<<1**.

<font size=3 >

**If $|U_3| ≥|U_2|$ and $|U_3| ≥ |U_1|$, deliver $U_2$.**

To simplify things, we first consider the distribution of the $|U_2|$. 

$$|U_i|\sim \mathbf{Uniform}(0,1)$$

By the assumption1 above, we can get that the random variable is either the $|U|_{(2)}$ (when $|U_2|\geq |U_1|$) or the$|U|_{(1)}$ (when $|U_2| <|U_1|$), where the $|U|_{(1)},|U|_{(2)},|U|_{(3)}$ are the order statistic of the $|U|_{1},|U|_2,|U|_3$.

Because of the independent identically distribution, $P(|U_2|\geq |U_1|)=\frac{1}{2}$.

$$P(|U_2|=t||U_3| ≥|U_2| and|U_3| ≥ |U_1|)=\frac{1}{2}P(|U|_{(2)}=t)P(|U_2|\geq |U_1|)+\frac{1}{2}P(|U|_{(1)}=t)P(|U_2|<|U_1|)$$

$$P(|U|_{(2)}=t)=6t-6t^2 ,\quad P(|U|_{(1)}=t)=3(1-t)^2$$

$$P(|U_2|=t||U_3| ≥|U_2| and|U_3| ≥ |U_1|)=\frac{1}{2}(6t-6t^2+3(1-t)^2)=\frac{3}{2}-\frac{3}{2}t^2$$

Because $U_2 \sim \mathbf{Uniform}(-1,1)$, use the symmetry at 0, we get

$$\forall t\in[0,1] \quad P(U_2=t||U_3| ≥|U_2| and|U_3| ≥ |U_1|)=\frac{1}{2}P(|U_2|=t||U_3| ≥|U_2| and|U_3| ≥ |U_1|)=\frac{3}{4}-\frac{3}{4}t^2$$

$$\forall t\in[-1,0] \quad P(U_2=t||U_3| ≥|U_2| and|U_3| ≥ |U_1|)=\frac{1}{2}P(|U_2|=-t||U_3| ≥|U_2| and|U_3| ≥ |U_1|)=\frac{3}{4}-\frac{3}{4}(-t)^2 =\frac{3}{4}-\frac{3}{4}t^2 $$

<font size=4>

**<<2**.

<font size=3 >

**Otherwise deliver $U_3$.**

We can regard $U_3$ as $U_2^*$, and repeat the process of proof in **<<1**.


## Exercises 3.13

<font size=4>

* Question

<font size=3>

It can be shown that the mixture in **Exercise 3.12** has a Pareto distribution with cdf

$$F(y)=1-\left( \frac{\beta}{\beta+y}\right)^r,y\geq 0$$

(This is an alternative parameterization of the Pareto cdf given in **Exercise 3.3**.) Generate 1000 random observations from the mixture with $r = 4$ and $\beta = 2$. Compare the empirical and theoretical (**Pareto**) distributions by graphing the density histogram of the sample and superimposing the Pareto density curve.

<font size=4>

* Answer

<font size=3 >

We can get the inverse function and the density function of $F(x)$ $$F^{-1}(x)=\frac{\beta}{\sqrt[r]{1-x}}-\beta$$
$$f(x)=F'(x)=\frac{r}{\beta}\left( \frac{\beta}{\beta+y}\right)^{r+1}$$
```{r}
F_i=function(x,r,b){
  b1=b
  f2=(1-x)^(1/r)
  return(b1/f2-b1)
}

n <- 1000
u <- runif(n)

x <- F_i(u,4,2)
hist(x, density = 3,breaks=50, col="red",prob = TRUE) #density histogram of sample

f=function(x,a,b){
  x1=b/(x+b)
  return(a/b*x1^(a+1))
}

y <- seq(0,max(x),0.01)

lines(y, f(y,4,2),col='blue')
```

# Homework2

## Exercises 5.1

<font size=4>

* Question

<font size=3>

Compute a Monte Carlo estimate of

$$\int_{0}^{\pi/3} sint{\rm d}t$$

and compare your estimate with the exact value of the integral.

<font size=4>

* Answer

<font size=3>
1.Generate random variables $X_i$ uniformly distributed from 0 to $\pi/3$.

2.Calculate the mean of the $sin(X_i)$ and product $\pi/3$.

3.Calculate the $$\int_0^{\pi/3}sint{\rm d}t=-cos(\pi/3)+cos(0)=1-1/2=1/2$$
```{r}
n=10000
u <- runif(n,0,pi/3)

e1=pi/3*mean(sin(u))

print('Our estimator is:')
print(e1)

```


## Exercises 5.7

<font size=4>

* Question

<font size=3>

Refer to Exercise 5.6. Use a Monte Carlo simulation to estimate $θ$ by the
antithetic variate approach and by the simple Monte Carlo method. Compute
an empirical estimate of the percent reduction in variance using the antithetic variate. Compare the result with the theoretical value from Exercise 5.6.

<font size=4>

* Answer

<font size=3>

First, compute:

$$Var(e^{1-U}+e^{U})=2Var(e^U)+2Cov(e^U,e^{1-U})=2(\frac{e^2-1}{2}-(e-1)^2)+2(e-(e-1)^2)=0.0156$$

$$Var(e^U)=\frac{e^2-1}{2}-(e-1)^2=0.242$$
Compute the theoretical value-
$$\frac{\frac{1}{m}Var(e^U)-\frac{1}{2m} Var(e^{1-U}+e^{U})}{\frac{1}{m}Var(e^U)}=0.9678$$


```{r }
m=10000
r=runif(m)
r1=runif(m/2)
r2=1-r1

#antithetic variate approach
g12=(exp(r1)+exp(r2))/2
e12=mean(g12)
print("The antithetic variate approach:")
print(e12)

#simple Monte
e=mean(exp(r))
print("the simple Monte Carlo method")
print(e)

##Var
V=sum((exp(r)-mean(exp(r)))^2)/(m*(m-1))
V12=sum((g12-mean(g12))^2)/((m/2)*(m/2-1))

Vred=(V-V12)/V

print("the percent reduction in variance is:")
print(Vred)
```


## Exercises 5.11

<font size=4>

* Question

<font size=3>
If $\hat{θ}_1$ and $\hat{θ}_2$ are unbiased estimators of $θ$, and $\hat{θ}_1$ and $\hat{θ}_2$ are antithetic, we
derived that $c∗ = 1/2$ is the optimal constant that minimizes the variance of
$\hat{θ}_c= c\hat{θ}_1 + (1 − c)\hat{θ}_2$. Derive c∗ for the general case. That is, if $\hat{θ}_1$ and $\hat{θ}_2$.
are any two unbiased estimators of $θ$, find the value c∗ that minimizes the
variance of the estimator $\hat{θ}_c= c\hat{θ}_1 + (1 − c)\hat{θ}_2$ in equation (5.11). (c∗ will be
a function of the variances and the covariance of the estimators.)

<font size=4>

* Answer

<font size=3 >

$$Var(\hat{\theta}_c)=Var(\hat{\theta_2})+c^2Var(\hat{\theta_1}-\hat{\theta_2})+2cCov(\hat{\theta_2},\hat{\theta_1}-\hat{\theta_2})$$
when $Var(\hat{\theta_1}-\hat{\theta_2})>0$, 

$$c*=-\frac{Cov(\hat{\theta_2},\hat{\theta_1}-\hat{\theta_2})}{Var(\hat{\theta_1}-\hat{\theta_2})}$$
when $Var(\hat{\theta_1}-\hat{\theta_2})=0$, 

$$\because \forall c\in [0,1],\quad \hat{\theta_1}\equiv\hat{\theta_2},\quad \hat{\theta_1}-\hat{\theta_2}\equiv 0,\quad Cov(\hat{\theta_2},\hat{\theta_1}-\hat{\theta_2})\equiv0$$
$$\therefore Var(\hat{\theta_c})\equiv Var(\hat{\theta_2})$$

# Homework3

## Exercises 5.13

<font size=4>

* Question

<font size=3>

Find two importance functions f1 and f2 that are supported on (1, ∞) and
are ‘close’ to

$$g(x)=\int_{1}^{\infty}\frac{x^2}{\sqrt{2\pi}}e^{-x^2/2}{\rm d}x$$
<font size=4>

* Answer

<font size=3>
1.Let $f_1(x)=\sqrt{\frac{2}{\pi}}e^{-\frac{(x-1)^2}{2}},\quad x>1.$

2.Let $f_2(x)=e^{-(x-1)},\quad x>1$.

3.Calculate the $\hat{\theta}$, and the standard error of $\frac{g(X_i)}{f_j(X_i)}\quad j=1,2.$

```{r}
n=10000
#samples from f1
u1 <- rnorm(n, mean = 1, sd = 1)
u2=abs(u1-1)+1

#samples from f2
v1=rexp(n,1)
v2=v1+1

#density g
g=function(x){
  return((x^2)*exp(-x^2/2)/sqrt(2*pi))
}

#density f1
f1=function(x){
  return(sqrt(2/pi)*exp(-(x-1)^2/2))
}

#density f2
f2=function(x){
  return(exp(-x+1))
}

theta=matrix(0,1,2)
se=matrix(0,1,2)

theta[1,1]=mean(g(u2)/f1(u2))
theta[1,2]=mean(g(v2)/f2(v2))

print(theta)

se[1,1]=sd(g(u2)/f1(u2))
se[1,2]=sd(g(v2)/f1(v2))

print(se)
```

As it turns out, the standard error of $\frac{g(X_i)}{f_1(X_i)}$ smaller than the $\frac{g(X_i)}{f_2(X_i)}$.Because of 

$$\frac{g(x)}{f_1(x)}=\frac{x^2}{2}e^{\frac{1-2x}{2}} \quad \frac{g(x)}{f_2(x)}=\frac{x^2}{\sqrt{2\pi}}e^{\frac{-x^2+2x-2}{2}}$$
with $x\to \infty$, the value of $\frac{g(X_i)}{f_2(X_i)}$ change much more faster. And $\frac{g(X_i)}{f_1(X_i)}$ is closer to a constant c.

## Exercises 5.15

<font size=4>

* Question

<font size=3>
Obtain the stratified importance sampling estimate in Example 5.13 and compare it with the result of **Example 5.10**.

<font size=4>

* Answer

<font size=3>
We compare the stratified importance sampling estimate with the MC estimate.

$$g(x)=e^{-x}/(1+x^2),\quad \forall x\in(0,1) $$
$$f_j(x)=\frac{e^{-x}}{e^{-(j-1)/5}-e^{-j/5}},\quad x \in(\frac{j-1}{5},\frac{j}{5})$$

```{r }
M <- 10000
k <- 5 
r <- M/k 

S=c()
T2 <- numeric(k)
est <- matrix(0, 1, 2)
se <- matrix(0, 1, 2)
#g
g <- function(x) {exp(-x - log(1+x^2)) * (x > 0) * (x < 1)}
#f_j
f_j=function(x,j){(exp(-x))/(exp(-(j-1)/k)-exp(-j/k))* (x > 0) * (x < 1)}

#estimate from MC
est[1,1] <- mean(g(runif(M)))
se[1,1]= sd(g(runif(M)))

#estimate from the stratified importance sampling
for(j in 1:k){
  u <- runif(r) #f3, inverse transform method
  d=(exp(-(j-1)/k)-exp(-j/k))
  x=-log(exp(-(j-1)/k)-(u)*d)
  
  T2[j]=mean(g(x)/f_j(x,j))
  S=c(S,g(x)/f_j(x,j))
}

est[1,2] <- sum(T2)
se[1,2]= sd(S)

print(est)
print(se)

```

Our result of the stratified importance sampling estimate is better than the result of **Example 5.10**.

## Exercises 6.4

<font size=4>

* Question

<font size=3>
Suppose that $X_1, . . . , X_n$ are a random sample from a from a log normal distribution with unknown parameters. Construct a 95% confidence interval for the parameter $µ$. Use a Monte Carlo method to obtain an empirical estimate of the confidence level.

<font size=4>

* Answer

<font size=3>
We set $\log X_i\sim \mathbf{U}(u,\sigma^2). \quad u=0,1,2,3,4\quad \sigma=1.$

<font size=3>

```{r}
n=10000
a=0.05
Re=1000
U=c(0,1,2,3,4)
sigma=1

for (uc in U) {
UCl=matrix(0,Re,4)
i=0
while(i<Re){
i=i+1
x=rlnorm(n,uc,sigma)
x=log(x)
S=sqrt(sum((x-mean(x))^2)/(n-1))
t=qt(1-a/2, df=n-1)
l1=mean(x)-t*S/sqrt(n)
l2=mean(x)+t*S/sqrt(n)
I=0
if(l1<uc &l2>uc){I=1}
UCl[i,]=c(uc,l1,l2,I)
 }
print(apply(UCl,2,mean))
}

```

## Exercises 6.5

<font size=4>

* Question
<font size=3>

Suppose a 95% symmetric t-interval is applied to estimate a mean, but the
sample data are non-normal. Then the probability that the confidence interval covers the mean is not necessarily equal to 0.95. Use a Monte Carlo experiment to estimate the coverage probability of the t-interval for random samples of $\chi^2(2)$ data with sample size $n$ = 20. Compare your $t$-interval results with the simulation results in Example 6.4. (The $t$-interval should be more robust to departures from normality than the interval for variance.)

<font size=4>

* Answer

<font size=3>

$$X\sim \chi^2(2)\quad u=\mathbf{E}(X)=2$$

```{r}
n=20
a=0.05
Re=1000
UCl=matrix(0,Re,3)
i=0
u=2

while(i<Re){
i=i+1
x=rchisq(n,u)
S=sqrt(sum((x-mean(x))^2)/(n-1))
t=qt(1-a/2, df=n-1)
l1=mean(x)-t*S/sqrt(n)
l2=mean(x)+t*S/sqrt(n)
I=0
if(l1<u &l2>u){
  I=1
}
UCl[i,]=c(I,l1,l2)
}

apply(UCl,2,mean)
```
Compare with the result of the **Example 6.6**, we know that $t$-interval is more robust to departures from normality than the interval for variance.

# Homework4

## Exercises 6.7

<font size=4>

* Question
<font size=3>

Estimate the power of the skewness test of normality against symmetric $Beta(α, α)$ distributions and comment on the results. Are the results different for heavy-tailed symmetric alternatives such as $t(ν)$?

<font size=4>

* Answer
<font size=3>

use the method like the text book
```{r,beta}

set.seed(12345)

sk = function(x) {
  xbar = mean(x)
  m3 = mean((x - xbar)^3)
  m2 = mean((x - xbar)^2)
  return( m3 / m2^1.5 )
}

# beta(a,a)
pwr_beta = function(a){
 alpha = 0.1
 n = 20
 m = 1e4
 N = length(a)
 pwr = numeric(N)
 cv = qnorm(1-alpha/2, 0, sqrt(6*(n-2) / ((n+1)*(n+3))))
 
 for (j in 1:N) { 
  sktests = numeric(m)
  for (i in 1:m) { 
   x = rbeta(n, a[j], a[j])
   sktests[i] = as.integer(abs(sk(x))>= cv)
  }
  pwr[j] = mean(sktests)
 }
 se = sqrt(pwr * (1-pwr) / m) 
 return(list(pwr = pwr,se = se))
}

 a = c(seq(0,1,0.1),seq(1,20,1),seq(20,100,10))
 pwr = pwr_beta(a)$pwr
 # plot the power
 se = pwr_beta(a)$se
 plot(a, pwr, type = "b", xlab = "a", ylab = "pwr", pch=16)
 abline(h = 0.1, lty = 2)
 lines(a, pwr+se, lty = 4)
 lines(a, pwr-se, lty = 4)
```

The power of the skewness test of normality against symmetric Beta(a,a) distribution is under 0.1. For a< 1, the empirical power is more and more far away from 0.1 With the increase of a. And for a>1, the empirical power increases to 0.1 when a increases. 

```{r,t}

# t(v)
pwr_t = function(v){
 
 alpha = 0.1
 n = 20
 m = 1e3
 N = length(v)
 pwr = numeric(N)
 cv = qnorm(1-alpha/2, 0, sqrt(6*(n-2) / ((n+1)*(n+3))))
 
 for (j in 1:N) { 
  sktests = numeric(m)
  for (i in 1:m) { 
   x = rt(n,v[j])
   sktests[i] = as.integer(abs(sk(x))>= cv)
  }
  pwr[j] = mean(sktests)
 }
 se = sqrt(pwr*(1-pwr) / m) 
  return(list(pwr = pwr,se = se))
}

v = seq(1,20)
pwr = pwr_t(v)$pwr
se = pwr_t(v)$se
# plot the power
plot(v, pwr, type = "b", xlab = "v", ylab = "pwr", ylim = c(0,1),pch=16)
abline(h = 0.1, lty = 2)
lines(v, pwr+se, lty = 4)
lines(v, pwr-se, lty = 4)

```

The results are different for heavy-tailed symmetric alternatives. For t distribution, the empirical power is always bigger than 0.1 and it decreases to 0.1 with the increase of v.


## Exercises 6.8

<font size=4>

* Question
<font size=3>

Refer to Example 6.16. Repeat the simulation, but also compute the F test of equal variance, at significance level $\hat{\alpha} \dot{=}0.055 $ Compare the power of the Count Five test and F test for small, medium, and large sample sizes. (Recall that the F test is not applicable for non-normal distributions.)

<font size=4>

* Answer
<font size=3>

```{r}
count5test <- function(x, y) {
        X <- x - mean(x)
        Y <- y - mean(y)
        outx <- sum(X > max(Y)) + sum(X < min(Y))
        outy <- sum(Y > max(X)) + sum(Y < min(X))
        return(as.integer(max(c(outx, outy)) > 5))
}
set.seed(12345)
alpha.hat <- 0.055
n <- c(10, 20, 50, 100, 500, 1000)
mu1 <- mu2 <- 0
sigma1 <- 1
sigma2 <- 1.5
m <- 1e4
result <- matrix(0, length(n), 2)
for (i in 1:length(n)){
  ni <- n[i]
  tests <- replicate(m, expr={
    x <- rnorm(ni, mu1, sigma1)
    y <- rnorm(ni, mu2, sigma2)
    Fp <- var.test(x, y)$p.value
    Ftest <- as.integer(Fp <= alpha.hat)
    c(count5test(x, y), Ftest)
    })
  result[i, ] <- rowMeans(tests)
}
data.frame(n=n, C5=result[, 1], Fp=result[, 2])

```
The simulation results suggest that the F-test for equal variance is more powerful in this case, for all sample sizes compared.


## Exercises 6.C

<font size=4>

* Question
<font size=3>

Repeat Examples 6.8 and 6.10 for Mardia’s multivariate skewness test. Mardia  proposed tests of multivariate normality based on multivariate generalizations of skewness and kurtosis. If X and Y are iid, the multivariate
population skewness $\beta_{1,d}$ is defined by Mardia as
$$\beta_{1,d}=E[(X-\mu)^T \Sigma^{-1}(Y-\mu)]^3$$
Under normality,$\beta_{1,d}=0$ . The multivariate skewness statistic is
$$b_{1,d}=\frac{1}{n^2} \sum_{i,j=1}^n {((X_i-\bar{X})^T \hat{\Sigma}^{-1} (X_j-\bar{X}))^3}$$
where $\hat{\Sigma}$is the maximum likelihood estimator of covariance. Large values of $b_{1,d}$ are significant. The asymptotic distribution of $\frac{nb_{1,d}}{6}$ is chisquared with $\frac{d(d + 1)(d + 2)}{6}$ degrees of freedom.

<font size=4>

* Answer
<font size=3>

We first repeat Example 6.8 which evaluate t1e rate of Mardia’s multivariate skewness test. In our simulation we generate variables following $N(\mu,\Sigma)$, where:
\[\mu=(0,0,0)^{T} , \Sigma=\left( \begin{array}{ccc}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1 \end{array} \right).\]
```{r}
library(MASS)
Mardia<-function(mydata){
  n=nrow(mydata)
  c=ncol(mydata)
  central<-mydata
  for(i in 1:c){
    central[,i]<-mydata[,i]-mean(mydata[,i])
  }
  sigmah<-t(central)%*%central/n
  a<-central%*%solve(sigmah)%*%t(central)
  b<-sum(colSums(a^{3}))/(n*n)
  test<-n*b/6
  chi<-qchisq(0.95,c*(c+1)*(c+2)/6)
  as.integer(test>chi)
}

set.seed(1234)
mu <- c(0,0,0)
sigma <- matrix(c(1,0,0,0,1,0,0,0,1),nrow=3,ncol=3)
m=1000
n<-c(10, 20, 30, 50, 100, 500)
#m: number of replicates; n: sample size
a=numeric(length(n))
for(i in 1:length(n)){
  a[i]=mean(replicate(m, expr={
    mydata <- mvrnorm(n[i],mu,sigma) 
    Mardia(mydata)
  }))
}
```

We calculate the t1e when the sample size is 10, 20, 30, 50, 100, 500: 
```{r}
print(a)
```
From the result we can see that t1e rate is close to 0.05 after the sample size is large than 50.


We further repeat Example 6.8 which evaluate the power of Mardia’s multivariate skewness test under distribution $(1-\epsilon)N(\mu_{1},\Sigma_{1})+\epsilon N(\mu_{2},\Sigma_{2})$, where:
\[\mu_{1}=\mu_{2}=(0,0,0)^{T}, \Sigma_{1}=\left( \begin{array}{ccc}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1 \end{array} \right)
\Sigma_{2}=\left( \begin{array}{ccc}
100 & 0 & 0 \\
0 & 100 & 0 \\
0 & 0 & 100 \end{array} \right).\]
```{r}
library(MASS)
set.seed(7912)
set.seed(7912)
mu1 <- mu2 <- c(0,0,0)
sigma1 <- matrix(c(1,0,0,0,1,0,0,0,1),nrow=3,ncol=3)
sigma2 <- matrix(c(100,0,0,0,100,0,0,0,100),nrow=3,ncol=3)
sigma=list(sigma1,sigma2)
m=1000
n=50
#m: number of replicates; n: sample size
epsilon <- c(seq(0, .06, .01), seq(.1, 1, .05))
N <- length(epsilon)
pwr <- numeric(N)
for (j in 1:N) { #for each epsilon
  e <- epsilon[j]
  sktests <- numeric(m)
  for (i in 1:m) { #for each replicate
    index=sample(c(1, 2), replace = TRUE, size = n, prob = c(1-e, e))
    mydata<-matrix(0,nrow=n,ncol=3)
    for(t in 1:n){
      if(index[t]==1) mydata[t,]=mvrnorm(1,mu1,sigma1) 
      else mydata[t,]=mvrnorm(1,mu2,sigma2)
    }
    sktests[i] <- Mardia(mydata)
  }
  pwr[j] <- mean(sktests)
}
plot(epsilon, pwr, type = "b",
     xlab = bquote(epsilon), ylim = c(0,1))
abline(h = .05, lty = 3)
se <- sqrt(pwr * (1-pwr) / m) #add standard errors
lines(epsilon, pwr+se, lty = 3)
lines(epsilon, pwr-se, lty = 3)
```

When $\epsilon=0$ or $\epsilon=1$ the distribution is multinormal, when $0\leq \epsilon \leq 1$ the
empirical power of the test is greater than 0.05 and highest(close to 1) when $0.1\leq \epsilon \leq 0.3$.


## Discussion

If we obtain the powers for two methods under a particular
simulation setting with 10,000 experiments: say, 0.651 for one method and 0.676 for another method. Can we say the powers are different at 0.05 level?

- What is the corresponding hypothesis test problem?
- What test should we use? Z-test, two-sample t-test, paired-t test or McNemar test?
- What information is needed to test your hypothesis?

<font size=4>

* Answer
<font size=3>

(1) Denote the powers of two methods as $pwr_{1}$ and $pwr_{2}$, then the corresponding hypothesis test problem is:
$$H_{0}: pwr_{1}=pwr_{2} \leftrightarrow H_{1}: pwr_{1}\not=pwr_{2}.$$

(2) As the p-value of two methods for the same sample is not independent, we can not apply the two-sample t-test. For the z-test and paired-t test, when the sample size is large, we have the mean value of significance test follows a normal distribution, thus these two methods can be used in the approximate level. McNemar test is good at dealing with this case as it doesn't need to know the distribution.

(3) For these test, what we already know is the number of experiments and the value of power(the probability that we reject the null hypothesis correctly). To conduct this test, we also need to know the significance of both methods for each sample. 

# Homework5

## Exercises 7.1

<font size=4>

* Question

<font size=3>

Compute a jackknife estimate of the bias and the standard error of the correlation statistic in **Example 7.2**.

<font size=4>

* Answer

<font size=3>
The law school data set law in the bootstrap package is from Efron and
Tibshirani. The data frame contains LSAT (average score on law school
admission test score) and GPA (average undergraduate grade-point average)
for 15 law schools.

We use Jackknife to estimate the bias (bias) and the standard
error of the correlation statistic (se) computed from the sample of scores in law.

```{r}
library(bootstrap) #for the law data

n <- nrow(law) #sample size
#set up the jackknife
#jackknife estimate of standard error of R

theta.jack=numeric(n)

for (i in 1:n){
LAST=law$LSAT[-i]
GPA=law$GPA[-i]
theta.jack[i]=cor(LAST,GPA)
} 

theta.hat=cor(law$LSAT, law$GPA)

bias <- (n - 1) * (mean(theta.jack) - theta.hat)

se <- sqrt((n-1) *mean((theta.jack - mean(theta.jack))^2))

#output
print(bias)
print(se)
```


## Exercises 7.5

<font size=4>

* Question

<font size=3>

Refer to **Exercise 7.4.** Compute 95% bootstrap confidence intervals for the
mean time between failures $1/\lambda$ by the standard normal, basic, percentile, and BCa methods. Compare the intervals and explain why they may differ.

<font size=4>

* Answer

<font size=3>

```{r }
#load data set
library(boot)
hour=aircondit$hours

#the mean time function
Mean=function(x,i){mean(x[i])}

#confidence intervals
boot.obj <- boot(hour,statistic = Mean, R = 2000)
print(boot.ci(boot.obj, type=c("basic","norm","perc","bca")))

```
Factors that may lead to different intervals:

* 1.the number of samples $n$ is small

* 2.the samples are not normally distributed

* 3.the variance $\frac{1}{\lambda^2}$ is related to $\theta=\frac{1}{\lambda}$


## Exercises 7.8

<font size=4>

* Question

<font size=3>

Refer to **Exercise 7.7**. Obtain the jackknife estimates of bias and standard error of $\hat{\theta}$

<font size=4>

* Answer

<font size=3>

We use Bootstrap and Jackknife to estimate the bias (bias) and the standard
error of the correlation statistic (se) computed from the sample of scores in law.

```{r}
library(bootstrap)

#The ordinary parameter estimation
Eigenvalue0=eigen(cov(scor))$values
theta0=Eigenvalue0[1]/sum(Eigenvalue0)
#The bootstrap estimation
B=200
n=nrow(scor)
theta=numeric(B)
for(b in 1:B){
i=sample(1:n,size=n,replace=TRUE)
A=cbind(scor$mec[i],scor$vec[i],scor$alg[i],scor$ana[i],scor$sta[i])
Sigma=cov(A)
Eigenvalue=eigen(Sigma)$values
theta[b]=Eigenvalue[1]/sum(Eigenvalue)
}
##Bootstrap estimate of se
print(sd(theta))
##Bootstrap estimate of bias
bias=mean(theta)-theta0
print(bias)

#The Jackknife estimation
theta2=numeric(n)
for(i in 1:n){
x=scor[-i,]
Eigenvalue=eigen(cov(x))$values
theta2[i]=Eigenvalue[1]/sum(Eigenvalue)}
##jackknife estimate of se
se= sqrt((n-1)*mean((theta2 - mean(theta2))^2))
print(se)

##jackknife estimate of bias
bias2=(n-1)*(mean(theta2)-theta0)
print(bias2)

```

## Exercises 7.11

<font size=4>

* Question

<font size=3>

In **Example 7.18**, leave-one-out (n-fold) cross validation was used to select the
best fitting model. Use leave-two-out cross validation to compare the models.

<font size=4>

* Answer

<font size=3>

Use leave-two-out cross validation, there are $\mathsf{C}^2_n=\frac{n(n-1)}{2}$ experiments with different pairs $i,j$.

In each experiment, we reported the Euclidean distance between the 2-dimension vector as the prediction error . $$error_{ij}=\sqrt{(\overrightarrow{y}[i]-\hat{y}_i)^2+(\overrightarrow{y}[j]-\hat{y}_j)^2},\quad i>j$$

Finally, we take the mean of the square of prediction error to get the optimal method.

<font size=3>
```{r}
library(DAAG)
attach(ironslag)

a <- seq(10, 40, .1)
n <- length(magnetic) #in DAAG ironslag
e1=c() 
e2=c() 
e3=c() 
e4=c() 
# for n-fold cross validation
# fit models on leave-one-out samples

for (i in 1:(n-1)){
for (j in (i+1):n) {

k=c(i,j)  
y <- magnetic[-k]
x <- chemical[-k]
J1 <- lm(y ~ x)
yhat1 <- J1$coef[1] + J1$coef[2] * chemical[k]
e1=c(e1,sqrt(sum((magnetic[k] - yhat1)^2)))

J2 <- lm(y ~ x + I(x^2))
yhat2 <- J2$coef[1] + J2$coef[2] * chemical[k] +
J2$coef[3] * chemical[k]^2
e2=c(e2,sqrt(sum((magnetic[k] - yhat2)^2)))

J3 <- lm(log(y) ~ x)
logyhat3 <- J3$coef[1] + J3$coef[2] * chemical[k]
yhat3 <- exp(logyhat3)
e3=c(e3,sqrt(sum((magnetic[k] - yhat3)^2)))

J4 <- lm(log(y) ~ log(x))
logyhat4 <- J4$coef[1] + J4$coef[2] * log(chemical[k])
yhat4 <- exp(logyhat4)
e4=c(e4,sqrt(sum((magnetic[k] - yhat4)^2)))
}}

c(mean(e1^2), mean(e2^2), mean(e3^2), mean(e4^2))

```

According to the prediction error criterion, the mean of error about the Model 2, the quadratic model, is the smallest, so it 
would be the best fit for the data.


# Homework6

## Exercises 8.3

<font size=4>

* Question

<font size=3>

The Count 5 test for equal variances in Section 6.4 is based on the maximum number of extreme points. **Example 6.15** shows that the Count 5 criterion is not applicable for unequal sample sizes. Implement a permutation test for equal variance based on the maximum number of extreme points that applies when sample sizes are not necessarily equal.

<font size=4>

* Answer

<font size=3>

We repeat $m=1000$ simulation experiments. In each experiment, we implement a permutation test for unequal sample sizes $(n_1=30,n_2=20)$ with $R=1000$. And samples $X_i$, $Y_j$ both $\sim N(0,1)$.

```{r}
maxout <- function(x, y) {
X <- x - mean(x)
Y <- y - mean(y)
outx <- sum(X > max(Y)) + sum(X < min(Y))
outy <- sum(Y > max(X)) + sum(Y < min(X))
# return 1 (reject) or 0 (do not reject H0)
return((max(c(outx, outy))))
}

n1 <- 30
n2 <- 20
mu1 <- mu2 <- 0
sigma1 <- sigma2 <- 1

P=numeric()
m=0
while (m<1000) {
x <- rnorm(n1, mu1, sigma1)
y <- rnorm(n2, mu2, sigma2)
z=c(x,y)
R=1000

D0=maxout(x, y)

D=numeric(R)
options(warn = -1)
for (i in 1:R) {
#generate indices k for the first sample
k <- sample((1:(n1+n2)), size = n1, replace = FALSE)
x1 <- z[k]
y1 <- z[-k] #complement of x1
D[i] <-maxout(x1,y1)
}
p <- mean(c(D0, D) >= D0)
P=c(P,p)
m=m+1
}
mean(P)

options(warn = 0)
```


## Discussion

Design experiments for evaluating the performance of the NN,
energy, and ball methods in various situations.

<font size=4>

* Question

<font size=3>

* 1.Unequal variances and equal expectations

* 2.Unequal variances and unequal expectations

* 3.Non-normal distributions: t distribution with 1 df (heavy-tailed
distribution), bimodel distribution (mixture of two normal
distributions)

* 4.Unbalanced samples (say, 1 case versus 10 controls)

* Note: The parameters should be chosen such that the powers
are distinguishable (say, range from 0.3 to 0.8).

<font size=4>

* Answer

<font size=3>

* 1. $X\sim N(0,1), Y\sim N(0,1.5^2)$

* 2. $X\sim N(0.5,1), Y\sim N(0,1.5^2)$

* 3. $X\sim t_1, Y\sim \frac{1}{2}N(0,1)+\frac{1}{2}N(0.5,1)$

* 4. $X\sim N(0,1), Y\sim N(0,1.5^2), n_1=\frac{1}{10}n_2$
```{r}
library(RANN) # implementing a fast algorithm
library(boot)

#NN_test
Tn <- function(z, ix, sizes,k) {
n1 <- sizes[1]
n2 <- sizes[2] 
n <- n1 + n2
if(is.vector(z)) z <- data.frame(z,0)
z <- z[ix, ]
NN <- nn2(data=z, k=k+1) # what's the first column?
block1 <- NN$nn.idx[1:n1,-1]
block2 <- NN$nn.idx[(n1+1):n,-1]
i1 <- sum(block1 < n1 + .5)
i2 <- sum(block2 > n1+.5)
(i1 + i2) / (k * n)
}

#Energy test
library(energy)

#Ball test
library(Ball)

#experiment 1
m <- 1e3
k<-3
p<-2
set.seed(12345)
n1 <- n2 <- 50
R<-999
n <- n1+n2
N = c(n1,n2)
eqdist.nn <- function(z,sizes,k){
boot.obj <- boot(data=z,statistic=Tn,R=R,
sim = "permutation", sizes = sizes,k=k)
ts <- c(boot.obj$t0,boot.obj$t)
p.value <- mean(ts>=ts[1])
list(statistic=ts[1],p.value=p.value)
}
p.values <- matrix(NA,m,3)
for(i in 1:m){
x <- matrix(rnorm(n1*p,0,1.5),ncol=p)
y <- matrix(rnorm(n2*p,0,1),ncol=p)
z <- rbind(x,y)
p.values[i,1] <- eqdist.nn(z,N,k)$p.value
p.values[i,2] <- eqdist.etest(z,sizes=N,R=R)$p.value
p.values[i,3] <- bd.test(x=x,y=y,R=999,seed=i*12345)$p.va
}

alpha <- 0.05;
pow <- colMeans(p.values<alpha)
pow

#experiment 2
p.values <- matrix(NA,m,3)
for(i in 1:m){
x <- matrix(rnorm(n1*p,0,1.5),ncol=p)
y <- matrix(rnorm(n2*p,0.5,1),ncol=p)
z <- rbind(x,y)
p.values[i,1] <- eqdist.nn(z,N,k)$p.value
p.values[i,2] <- eqdist.etest(z,sizes=N,R=R)$p.value
p.values[i,3] <- bd.test(x=x,y=y,R=999,seed=i*12345)$p.va
}

alpha <- 0.05;
pow <- colMeans(p.values<alpha)
pow

#experiment 3

p.values <- matrix(NA,m,3)
for(i in 1:m){
x <- matrix(rt(n1*p,1),ncol=p)

y1=c(rnorm((n2/2),0,1),rnorm((n2/2),0.5,1))
y <- matrix(c(y1,y1),ncol=p)
z <- rbind(x,y)
p.values[i,1] <- eqdist.nn(z,N,k)$p.value
p.values[i,2] <- eqdist.etest(z,sizes=N,R=R)$p.value
p.values[i,3] <- bd.test(x=x,y=y,R=999,seed=i*12345)$p.va
}

alpha <- 0.05;
pow <- colMeans(p.values<alpha)
pow

#experiment 4
n1=10
n2=100
n <- n1+n2
N = c(n1,n2)
p.values <- matrix(NA,m,3)
for(i in 1:m){
x <- matrix(rnorm(n1*p,0,1.5),ncol=p)
y <- matrix(rnorm(n2*p,0,1),ncol=p)
z <- rbind(x,y)
p.values[i,1] <- eqdist.nn(z,N,k)$p.value
p.values[i,2] <- eqdist.etest(z,sizes=N,R=R)$p.value
p.values[i,3] <- bd.test(x=x,y=y,R=999,seed=i*12345)$p.va
}

alpha <- 0.05
pow <- colMeans(p.values<alpha)
pow
```

# Homework7

## Exercises 9.4

<font size=4>

* Question

<font size=3>

Implement a random walk Metropolis sampler for generating the standard
Laplace distribution (see **Exercise 3.2**). For the increment, simulate from a
normal distribution. Compare the chains generated when different variances
are used for the proposal distribution. Also, compute the acceptance rates of
each chain.

<font size=4>

* Answer

<font size=3>

The standard Laplace distribution has density 

$$f(x) = \frac{1}{2}e^{-|x|}, x \in \mathbb{R}.$$ 

```{r}
library('xtable')
#density of the standard Laplace distribution
f=function(x){return(0.5*exp(-abs(x)))}

rw.Metropolis <- function(sigma, x0, N) {
x <- numeric(N)
x[1] <- x0
u <- runif(N)
k <- 0
for (i in 2:N) {
y <- rnorm(1, x[i-1], sigma)
if (u[i] <= (f(y) / f(x[i-1])))
x[i] <- y 
else {
x[i] <- x[i-1]
k <- k + 1
}
}
return(list(x=x, k=k))
}

N <- 2000
sigma <- c(.05, .5, 2, 16)
x0 <- 25
rw1 <- rw.Metropolis(sigma[1], x0, N)
rw2 <- rw.Metropolis(sigma[2], x0, N)
rw3 <- rw.Metropolis(sigma[3], x0, N)
rw4 <- rw.Metropolis(sigma[4], x0, N)
#number of candidate points rejected
print(c(rw1$k/N, rw2$k/N, rw3$k/N, rw4$k/N))

print(c(mean(rw1$x[501:N]),mean(rw2$x[501:N]),mean(rw3$x[501:N]),mean(rw4$x[501:N])))

plot(rw1$x,type = "l",ylab = "x",xlab="sigma=0.05")
plot(rw2$x,type = "l",ylab = "x",xlab="sigma=0.5")
plot(rw3$x,type = "l",ylab = "x",xlab="sigma=2")
plot(rw4$x,type = "l",ylab = "x",xlab="sigma=16")

```


## Exercises 9.4+

<font size=4>

* Question

<font size=3>

For Exercise 9.4, use the Gelman-Rubin method to monitor
convergence of the chain, and run the chain until it converges
approximately to the target distribution according to $\hat{R} < 1.2$.

<font size=4>

* Answer

<font size=3>

The standard Laplace distribution has density 

$$f(x) = \frac{1}{2}e^{-|x|}, x \in \mathbb{R}.$$ 

```{r}
f=function(x){return(0.5*exp(-abs(x)))}

Gelman.Rubin <- function(psi) {
# psi[i,j] is the statistic psi(X[i,1:j])
# for chain in i-th row of X
psi <- as.matrix(psi)
n <- ncol(psi)
k <- nrow(psi)
psi.means <- rowMeans(psi) #row means
B <- n * var(psi.means) #between variance est.
psi.w <- apply(psi, 1, "var") #within variances
W <- mean(psi.w) #within est.
v.hat <- W*(n-1)/n + (B/n) #upper variance est.
r.hat <- v.hat / W #G-R statistic
return(r.hat)
}

Laplace.chain <- function(sigma, N, X1) {
#generates a Metropolis chain for Laplace distribution
#with Normal(X[t], sigma) proposal distribution
#and starting value X1
x <- rep(0, N)
x[1] <- X1
u <- runif(N)
for (i in 2:N) {
xt <- x[i-1]
y <- rnorm(1, xt, sigma) #candidate point
r <- f(y) / f(x[i-1])
if (u[i] <= r) x[i] <- y else
x[i] <- xt
}
return(x)
}

sigma <- 1 #parameter of proposal distribution
k <- 4 #number of chains to generate
n <- 10000 #length of chains
b <- 500 #burn-in length
#choose overdispersed initial values
x0 <- c(-10, -5, 5, 10)
#generate the chains
X <- matrix(0, nrow=k, ncol=n)
for (i in 1:k)
X[i, ] <- Laplace.chain(sigma, n, x0[i])
#compute diagnostic statistics
psi <- t(apply(X, 1, cumsum))
for (i in 1:nrow(psi))
psi[i,] <- psi[i,] / (1:ncol(psi))
print(Gelman.Rubin(psi))
#plot psi for the four chains

for (i in 1:k)
plot(psi[i, (b+1):n], type="l",
xlab=i, ylab=bquote(psi))
par(mfrow=c(1,1)) #restore default
#plot the sequence of R-hat statistics
#plot the sequence of R-hat statistics

rhat <- rep(0, n)
for (j in (b+1):n)
rhat[j] <- Gelman.Rubin(psi[,1:j])
plot(rhat[(b+1):n], type="l", xlab="", ylab="R")
abline(h=1.2, lty=2)

```

## Exercises 11.4

<font size=4>

* Question

<font size=3>

Find the intersection points $A(k)$ in $(0, \sqrt{k})$ of the curves

$$ S_{k-1}(a)=P\left(t(k-1)>\sqrt{\frac{a^2(k-1)}{k-a^2}}\right)$$
and 
$$S_k(a)=P\left(t(k)>\sqrt{\frac{a^2k}{k+1-a^2}}\right)$$
for $k = 4 : 25, 100, 500, 1000$, where $t(k)$ is a Student t random variable with
$k$ degrees of freedom. (These intersection points determine the critical values
for a $t$-test for scale-mixture errors proposed by Szekely [260].)

<font size=4>

* Answer

<font size=3>

$$f(a)=S_k(a)-S_{k-1}(a)$$
Use Brent’s method to find the root of $f(a)$.
```{r}
options(warn = -1)
f=function(k,a){
  p1=sqrt(a^2*k/(k+1-a^2))
  p2=sqrt(a^2*(k-1)/(k-a^2))
  return(pt(p1,k)-pt(p2,k-1))
}

K=c(4:25,100,500,1000)
Result=c()
j=0
for (i in K) {
f_1=function(y){return(f(i,y))}
out<- uniroot(function(y){return(f(i,y))},interval =c(0.01,min(sqrt(i)-0.01,4)))
if(j==1){curve(f_1(x),0,sqrt(i),add = T,xlim = c(0,5))}
else{curve(f_1(x),0,sqrt(i),xlim = c(0,5),ylab = "Sk-S(k-1)",xlab="a")}
Result=c(Result,out$root)
j=1
}
abline(h = 0, col = "red")
Result

```

# Homework8

## Ex1

<font size=4>

* Question

<font size=3>

$$n_A.=n_{AA}+n_{AO}=444;\quad n_B.=n_{BB}+n_{BO}=132;\quad n_{OO}=361;\quad n_{AB}=63$$

<font size=4>

* Answer

<font size=3>

Set the number of cycles $M=1000$. 

Set the convergence criteria $R=\frac{|p^{(t)}-p^{(t+1)}|}{|p^{(t+1)}|}$. When the $R<5\times 10^{-20}$, break the cycle.

**E-step:**

$$\hat{n}^{(t)}_{AA}=n_A\times \frac{{[p^{(t)}]}^2}{(2\times r^{(t)}\times p^{(t)}+[p^{(t)}]^2)}\quad
\hat{n}^{(t)}_{BB}=n_B\times \frac{[q^{(t)}]^2}{(2\times r^{(t)}\times q^{(t)}+[q^{(t)}]^2)}$$


$$\hat{n}^{(t)}_{AO}=2n_A \frac{p^{(t)} r^{(t)}}{[p^{(t)}]^2+2 p^{(t)} r^{(t)}}\quad 
\hat{n}^{(t)}_{BO}=2n_B \frac{q^{(t)} r^{(t)}}{[q^{(t)}]^2+2 q^{(t)} r^{(t)}   }$$

Corresponding log-maximum likelihood=

$$L(\theta|n_A.,n_B.,n_{OO},n_{AB})=$$
$$n_A. \log ([p^{(t)}]^2+2 p^{(t)} r^{(t)})+n_B. \log ([q^{(t)}]^2+2 q^{(t)} r^{(t)})+n_{AB} \log(2 q^{(t)} p^{(t)})+2n_{OO}\log(r^{(t)})$$
**M-step:**

$$p^{(t+1)}=\frac{2n^{(t)}_{AA}+n^{(t)}_{AO}+n^{(t)}_{AB}}{2n}$$
$$q^{(t+1)}=\frac{2n^{(t)}_{BB}+n^{(t)}_{BO}+n^{(t)}_{AB}}{2n}$$
$$r^{(t+1)}=\frac{2n^{(t)}_{OO}+n^{(t)}_{BO}+n^{(t)}_{AO}}{2n}$$

```{r }
r0=0.4
p0=0.3
q0=0.3#The initial value

n_A=444
n_B=132
n_OO=361
n_AB=63
n=n_A+n_B+n_OO+n_AB

M=1000#times
m=0

rt=r0
pt=p0
qt=q0

while (m<M) {
m=m+1
Lt=(n_A*log(pt^2+2*pt*rt))+(n_B*log(qt^2+2*qt*rt))+(n_AB*log(2*pt*qt))+(2*n_OO*log(rt))
if(m==1){print(c('p','q','r','log-ml'))}

print(c(pt,qt,rt,Lt))

n_AAt=n_A*pt^2/(2*rt*pt+pt^2)
n_BBt=n_B*qt^2/(2*rt*qt+qt^2)
n_AOt=n_A*2*pt*rt/(pt^2+2*pt*rt)
n_BOt=n_B*2*qt*rt/(qt^2+2*qt*rt)

pt1=(2*n_AAt+n_AOt+n_AB)/(2*n)
qt1=(2*n_BBt+n_BOt+n_AB)/(2*n)
rt1=(2*n_OO+n_AOt+n_BOt)/(2*n)
Lt

R=abs(pt1-pt)/pt

if(R<5*exp(-20)){break}
pt=pt1
qt=qt1
rt=rt1

}

```

## Ex2

<font size=4>

* Question

<font size=3>

Use both for loops and $lapply()$ to fit linear models to the
mtcars using the formulas stored in this list


```{r }
data("mtcars")
m<-mtcars
attach(m)

formulas <- list(
mpg ~ disp,
mpg ~ I(1 / disp),
mpg ~ disp + wt,
mpg ~ I(1 / disp) + wt
)

for (i in formulas) {
 print(lm(i)) 
}

lapply(formulas, lm)

```

## Ex3

<font size=4>

* Question

<font size=3>

The following code simulates the performance of a $t$-test for
non-normal data. Use $sapply()$ and an anonymous function
to extract the $p$-value from every trial.

```{r}
trials <- replicate(
100,
t.test(rpois(10, 10), rpois(7, 10)),
simplify = FALSE
)

f=function(trial){
  return(trial$p.value)
}

sapply(trials, f)
```

## Ex4

<font size=4>

* Question

<font size=3>

Implement a combination of $Map()$ and $vapply()$ to create an
$lapply()$ variant that iterates in parallel over all of its inputs
and stores its outputs in a vector (or a matrix). What arguments should the function take?
```{r}

F_Mv<-function (f,n,type,...) {  
 NM=unlist(Map(f,...)) 
 if(type=="numeric") return(vapply(NM,cbind,numeric(n))) 
 else if (type=="character") return(vapply(NM,cbind,character(n))) 
 else if (type=="complex") return(vapply(NM,cbind,complex(n))) 
 else if (type=="logical") return(vapply(NM,cbind,logical(n))) 
 }

xs <- list(runif(10), runif(5)) # different element lengths
ws <- list(rpois(10, 2) + 1, rpois(5, 2) + 1)

str(F_Mv(weighted.mean,1,"numeric",xs,ws))

```

# Homework9
## Ex1

* Question

Write an Rcpp function for **Exercise 9.4** (page 277, Statistical
Computing with R)

* Answer

We write a function **rw.Metropolis** in R and a function **rwMC** in Rcpp. We consider compare the two function with different $\sigma\in(0.05,0.5,2,16)$.
Then we use **Sys.time()** record the run time. 

```{r }
library('xtable')
#density of the standard Laplace distribution
f=function(x){return(0.5*exp(-abs(x)))}

rw.Metropolis <- function(sigma, x0, N) {
x <- numeric(N)
x[1] <- x0
u <- runif(N)
k <- 0
for (i in 2:N) {
y <- rnorm(1, x[i-1], sigma)
if (u[i] <= (f(y) / f(x[i-1])))
x[i] <- y 
else {
x[i] <- x[i-1]
k <- k + 1
}
}
return(list(x=x, k=k))
}

N <- 10000
sigma <- c(.05, .5, 2, 16)
x0 <- 25

t1=Sys.time()
rw1 <- rw.Metropolis(sigma[1], x0, N)
t2=Sys.time()
rw2 <- rw.Metropolis(sigma[2], x0, N)
t3=Sys.time()
rw3 <- rw.Metropolis(sigma[3], x0, N)
t4=Sys.time()
rw4 <- rw.Metropolis(sigma[4], x0, N)
t5=Sys.time()

print(rbind(c(rw1$k/N,rw2$k/N,rw3$k/N,rw4$k/N),c(t2-t1,t3-t2,t4-t3,t5-t4)))
```

```{r}
library(Rcpp)
cppFunction('NumericMatrix rwMC(double sigma,double x0,int N) {
  NumericMatrix mat(N, 2);
  mat(0,0)=x0;
  mat(0,1)=0;
  double y=0,u=0;
  for(int i = 2; i < N+1; i++) {
    y=rnorm(1,mat(i-2,0),sigma)[0];
    u=runif(1,0,1)[0];
    if(u<=exp(-abs(y))/exp(-abs(mat(i-2,0)))){
      mat(i-1,0)=y;
      mat(i-1,1)=mat(i-2,1);
    }
    else{
      mat(i-1,0)=mat(i-2,0);
      mat(i-1,1)=mat(i-2,1)+1;
    }
  }
  return(mat);
}')

t1=Sys.time()
Rw1 <- rwMC(sigma[1], x0, N)
t2=Sys.time()
Rw2 <- rwMC(sigma[2], x0, N)
t3=Sys.time()
Rw3 <- rwMC(sigma[3], x0, N)
t4=Sys.time()
Rw4 <- rwMC(sigma[4], x0, N)
t5=Sys.time()

print(rbind(c(Rw1[N,2]/N,Rw2[N,2]/N,Rw3[N,2]/N,Rw4[N,2]/N),c(t2-t1,t3-t2,t4-t3,t5-t4)))

qqplot(Rw1[,1],rw1$x)
qqplot(Rw2[,1],rw2$x)
qqplot(Rw3[,1],rw3$x)
qqplot(Rw4[,1],rw4$x)
```

## Ex2 

* Answer

Compare the corresponding generated random numbers with
those by the R function you wrote before using the function
**“qqplot”**.

* Question

We use **runif** and **GetRandomNumber** which we wrote in Rcpp to random generate $N$ numbers.


```{r}
cppFunction('NumericVector GetRandomNumber(int N)
{
	NumericVector RandomNumber(N);
  int i;
  int b=10000;
  
srand((unsigned)time(NULL));
for(i=0;i<N;i++){
	RandomNumber(i) = rand() % b / (double)(b) ;
}
	return RandomNumber;
}')

N=1000
L1=GetRandomNumber(N)
L2=runif(N)

qqplot(L1,L2)

```

## Ex3

* Question

Campare the computation time of the two functions with the
function **“microbenchmark”**.

* Answer

We compare them with different $\sigma\in(0.05,0.5,2,16)$.

```{r}
library(microbenchmark)
ts <- microbenchmark(rw1=rw.Metropolis(sigma[1], x0, N),
Rw1=rwMC(sigma[1], x0, N),rw2=rw.Metropolis(sigma[2], x0, N),
Rw2=rwMC(sigma[2], x0, N),rw3=rw.Metropolis(sigma[3], x0, N),
Rw3=rwMC(sigma[3], x0, N),rw4=rw.Metropolis(sigma[4], x0, N),
Rw4=rwMC(sigma[4], x0, N))

summary(ts)[,c(1,3,5,6)]
```

## Comment

The function wrote in Rcpp runs much faster when the $N$ is large. So we can believe that Rcpp can speed up in data structures with lots of loops.